from __future__ import annotations

import argparse
import json
import os
import csv

import warnings
import logging

# Suppress ALL warnings before importing any libraries
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"

from .utils.logging import get_logger

# Get logger for CLI
logger = get_logger("RAG_CLI")

# Suppress external library warnings before importing pipeline
try:
    warnings.filterwarnings("ignore", category=UserWarning, module="jieba._compat")
    warnings.filterwarnings("ignore", message="pkg_resources is deprecated as an API", category=UserWarning)
    warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
    warnings.filterwarnings("ignore", category=FutureWarning, module="transformers")
    warnings.filterwarnings("ignore", message=".*torch_dtype.*is deprecated.*")
    warnings.filterwarnings("ignore", message=".*max_length.*is ignored.*")
except Exception:
    pass
try:
    logging.getLogger("jieba").setLevel(logging.ERROR)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("transformers").setLevel(logging.ERROR)
    logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
    try:
        import jieba  # type: ignore
        jieba.setLogLevel(logging.ERROR)
    except Exception:
        pass
except Exception:
    pass

from typing import List, Set
import re as regex_module
from .agent import RagAgent
# from .memory import rewrite_question
from .utils.logging import set_logging_debug_mode, is_logging_debug_mode
from .core.types import CitationInfo


def extract_cited_refs(text: str) -> Set[int]:
    """ä»æ–‡æœ¬ä¸­æå– [n] æ ¼å¼çš„å¼•ç”¨ç¼–å·ã€‚"""
    matches = regex_module.findall(r'\[(\d+)\]', text)
    return {int(m) for m in matches}


def format_citations(
    citation_infos: List[CitationInfo],
    cited_refs: Set[int],
    show_all: bool = False
) -> str:
    """æ ¼å¼åŒ–å¼•ç”¨è¾“å‡ºã€‚
    
    Args:
        citation_infos: æ‰€æœ‰å¼•ç”¨ä¿¡æ¯
        cited_refs: LLM å®é™…å¼•ç”¨çš„ç¼–å·é›†åˆ
        show_all: æ˜¯å¦æ˜¾ç¤ºæ‰€æœ‰å¼•ç”¨ï¼ˆè°ƒè¯•ç”¨ï¼‰
    
    Returns:
        æ ¼å¼åŒ–çš„å¼•ç”¨å­—ç¬¦ä¸²
    """
    lines = []
    lines.append("\n---")
    lines.append("**ğŸ“Š æ•°æ®æ¥æº (References)**\n")
    
    cited_count = 0
    uncited_count = 0
    
    for info in citation_infos:
        if info.ref in cited_refs:
            cited_count += 1
            
            # æ ¹æ®æ–‡æ¡£ç±»å‹ç¡®å®šç±»å‹æ ‡ç­¾
            if info.doc_type == "sql":
                type_tag = "[ç»“æ„åŒ–æ•°æ®]"
            elif info.doc_type == "table":
                type_tag = "[è¡¨æ ¼]"
            else:
                type_tag = "[æ–‡æœ¬]"
            
            # æ ¼å¼åŒ–é¡µç ä¿¡æ¯ï¼ˆä»…é SQL æ•°æ®æ˜¾ç¤ºï¼‰
            if info.doc_type != "sql" and info.page:
                page_tag = f" (Page: {info.page})"
            else:
                page_tag = ""
            
            lines.append(f"* **[{info.ref}]** {type_tag} {info.title}{page_tag}")
            lines.append("")
        else:
            uncited_count += 1
    
    if uncited_count > 0:
        lines.append(f"*(å·²è¿‡æ»¤ {uncited_count} æ¡æœªå¼•ç”¨çš„æ£€ç´¢æº)*")
    
    if cited_count == 0:
        lines.append("*(æœªæ£€æµ‹åˆ°å¼•ç”¨æ ‡è®°ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ£€ç´¢æº)*\n")
        for info in citation_infos:
            # æ ¹æ®æ–‡æ¡£ç±»å‹ç¡®å®šç±»å‹æ ‡ç­¾
            if info.doc_type == "sql":
                type_tag = "[ç»“æ„åŒ–æ•°æ®]"
            elif info.doc_type == "table":
                type_tag = "[è¡¨æ ¼]"
            else:
                type_tag = "[æ–‡æœ¬]"
            page_tag = f" (Page: {info.page})" if info.page and info.doc_type != "sql" else ""
            lines.append(f"* [{info.ref}] {type_tag} {info.title}{page_tag}")
    
    return "\n".join(lines)


def warmup_models():
    """é¢„çƒ­æ¨¡å‹ï¼šé¢„å…ˆåŠ è½½æœç´¢å¼•æ“ç»„ä»¶ã€Embedding æ¨¡å‹å’Œ Reranker æ¨¡å‹åˆ°ç¼“å­˜ã€‚
    
    è¿™æ ·åœ¨ç”¨æˆ·è¾“å…¥ç¬¬ä¸€ä¸ªé—®é¢˜æ—¶å°±å¯ä»¥ç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼Œæ— éœ€ç­‰å¾…æ¨¡å‹åŠ è½½ã€‚
    é¢„çƒ­å†…å®¹ï¼š
    1. LocalRetriever (Milvus + BM25 + SQL)
    2. Ollama Embedding æ¨¡å‹ (qwen3-embedding:4b)
    3. Reranker æ¨¡å‹ (Qwen3-Reranker-0.6B)
    """
    import time
    
    debug = is_logging_debug_mode()
    
    if debug:
        logger.info("é¢„çƒ­æ¨¡å‹ä¸­...")
    
    start_total = time.time()
    
    # é¢„çƒ­æœ¬åœ°æ··åˆæ£€ç´¢å™¨ï¼ˆåŒ…å« Milvus + BM25 + SQLï¼‰
    try:
        from .retrieval import get_retriever
        if debug:
            logger.debug("åŠ è½½ LocalRetriever (Milvus + BM25 + SQL)...")
        t0 = time.time()
        retriever = get_retriever()
        # è§¦å‘å†…éƒ¨ç»„ä»¶åˆå§‹åŒ–
        retriever.vector_searcher._ensure_client()
        retriever.bm25_searcher._ensure_loaded()
        if debug:
            logger.debug(f"LocalRetriever å°±ç»ª (took {time.time() - t0:.2f}s)")
    except Exception as e:
        if debug:
            logger.warning(f"LocalRetriever åŠ è½½å¤±è´¥: {e}")
    
    # é¢„çƒ­ Ollama Embedding æ¨¡å‹
    try:
        import ollama
        from .config import get_config
        config = get_config()
        if debug:
            logger.debug(f"é¢„çƒ­ Embedding æ¨¡å‹ ({config.ollama_embed_model})...")
        t0 = time.time()
        # ä½¿ç”¨ä¸€ä¸ªçŸ­æ–‡æœ¬è§¦å‘æ¨¡å‹åŠ è½½
        _ = ollama.embeddings(model=config.ollama_embed_model, prompt="é¢„çƒ­")
        if debug:
            logger.debug(f"Embedding æ¨¡å‹å°±ç»ª (took {time.time() - t0:.2f}s)")
    except Exception as e:
        if debug:
            logger.warning(f"Embedding æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # é¢„çƒ­ Reranker æ¨¡å‹ï¼ˆHuggingFace Qwen3-Rerankerï¼‰
    try:
        from .retrieval.rankers import SemanticReranker
        if debug:
            logger.debug("åŠ è½½ Reranker æ¨¡å‹ (Qwen3-Reranker-0.6B)...")
        t0 = time.time()
        reranker = SemanticReranker()
        # è§¦å‘æ¨¡å‹åŠ è½½
        reranker._load_model()
        if debug:
            logger.debug(f"Reranker æ¨¡å‹å°±ç»ª (took {time.time() - t0:.2f}s)")
    except Exception as e:
        if debug:
            logger.warning(f"Reranker æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    total_time = time.time() - start_total
    if debug:
        logger.info(f"é¢„çƒ­å®Œæˆï¼Œæ€»è€—æ—¶ {total_time:.2f}s")


def main():
    # å·²åœ¨æ¨¡å—å¯¼å…¥å‰è®¾ç½®å‘Šè­¦æŠ‘åˆ¶ä¸å¤–éƒ¨æ—¥å¿—çº§åˆ«ï¼Œè¿™é‡Œæ— éœ€é‡å¤
    parser = argparse.ArgumentParser(description="Run RAG Agent pipeline")
    # é—®é¢˜å‚æ•°æ”¹ä¸ºå¯é€‰ï¼šæœ‰åˆ™å•æ¬¡è¿è¡Œï¼Œæ— åˆ™è¿›å…¥äº¤äº’æ¨¡å¼
    parser.add_argument("question", type=str, nargs="?", help="User question")
    parser.add_argument("--trace-id", type=str, default=None, help="Optional trace id for logging")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤º pipeline æ¯å±‚çš„å½©è‰²è¯¦ç»†æ—¥å¿—")
    # æ‰¹é‡æ¨¡å¼ï¼šä»æ–‡ä»¶è¯»å–é—®é¢˜å¹¶å†™ JSONL è¾“å‡º
    parser.add_argument("--input", type=str, default=None, help="åŒ…å«é—®é¢˜çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼‰")
    parser.add_argument(
        "--output", type=str, default=os.path.join("outputs", "answers.jsonl"),
        help="æ‰¹é‡æ¨¡å¼è¾“å‡ºæ–‡ä»¶ï¼ˆJSONLï¼‰"
    )
    parser.add_argument("--append", action="store_true", help="æ‰¹é‡æ¨¡å¼å†™å…¥æ—¶è¿½åŠ åˆ°è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--enable-memory", action="store_true", help="äº¤äº’æ¨¡å¼å¼€å¯çŸ­æœŸè®°å¿†ï¼ˆLangGraphï¼‰")
    parser.add_argument("--thread-id", type=str, default=None, help="è®°å¿†ä¼šè¯IDï¼ˆé»˜è®¤ä½¿ç”¨ trace-id æˆ– 'repl'ï¼‰")
    args = parser.parse_args()

    # å¯ç”¨è°ƒè¯•æ¨¡å¼
    if args.debug:
        set_logging_debug_mode(True)

    agent = RagAgent(trace_id=args.trace_id)

    # æ‰¹é‡æ¨¡å¼ä¼˜å…ˆï¼šä»æ–‡ä»¶è¯»å–é—®é¢˜ï¼Œé€ä¸€ç”Ÿæˆç­”æ¡ˆå¹¶å†™å…¥ï¼ˆJSONL/CSV è‡ªåŠ¨è¯†åˆ«ï¼‰
    if args.input:
        in_path = os.path.abspath(args.input)
        out_path = os.path.abspath(args.output)
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)

        if not os.path.exists(in_path):
            logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{in_path}")
            return

        mode = "a" if args.append and os.path.exists(out_path) else "w"
        total = 0
        ok = 0
        err = 0
        # æ ¹æ®æ‰©å±•åé€‰æ‹©è¾“å‡ºæ ¼å¼
        is_csv = out_path.lower().endswith(".csv")
        with open(in_path, "r", encoding="utf-8") as fin:
            if is_csv:
                # CSV å†™å…¥ï¼ˆä¸ qa_with_refs.csv ä¸€è‡´çš„ä¸‰åˆ—è¡¨å¤´ï¼‰
                with open(out_path, mode, encoding="utf-8", newline="") as fout:
                    writer = csv.writer(fout)
                    if mode == "w":
                        writer.writerow(["é—®é¢˜", "æ ‡å‡†ç­”æ¡ˆ", "å¼•ç”¨æ¥æº"])  # å†™è¡¨å¤´
                    for line in fin:
                        q = (line or "").strip()
                        if not q:
                            continue
                        total += 1
                        try:
                            ans = agent.run(q)
                            citations_text = "; ".join([str(c) for c in (ans.citations or []) if str(c).strip()])
                            writer.writerow([q, ans.text, citations_text])
                            ok += 1
                        except Exception as e:
                            writer.writerow([q, f"é”™è¯¯ï¼š{e}", ""])
                            err += 1
            else:
                # JSONL å†™å…¥ï¼ˆé»˜è®¤ï¼‰
                with open(out_path, mode, encoding="utf-8") as fout:
                    for line in fin:
                        q = (line or "").strip()
                        if not q:
                            continue
                        total += 1
                        try:
                            ans = agent.run(q)
                            row = {
                                "question": q,
                                "answer": ans.text,
                                "citations": ans.citations,
                                "confidence": ans.confidence,
                                "meta": ans.meta,
                            }
                            fout.write(json.dumps(row, ensure_ascii=False) + "\n")
                            ok += 1
                        except Exception as e:
                            row = {"question": q, "error": str(e)}
                            fout.write(json.dumps(row, ensure_ascii=False) + "\n")
                            err += 1
        logger.info(f"å¤„ç†å®Œæˆï¼Œå…± {total} æ¡ï¼›æˆåŠŸ {ok}ï¼Œå¤±è´¥ {err}ã€‚è¾“å‡ºæ–‡ä»¶ï¼š{out_path}")
        return

    # å•æ¬¡è¿è¡Œï¼ˆæµå¼è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼‰
    if args.question:
        # å•æ¬¡è¿è¡Œä¹Ÿé¢„çƒ­ï¼Œè¿™æ ·ç¬¬ä¸€ä¸ªé—®é¢˜å°±èƒ½å¿«é€Ÿå“åº”
        warmup_models()
        stream, citation_infos = agent.run_stream(args.question)
        print("=== Final Answer ===")
        final_text_parts: List[str] = []
        for delta in stream:
            final_text_parts.append(delta)
            print(delta, end="", flush=True)
        final_text = "".join(final_text_parts)
        print()  # ensure newline after stream
        
        # è§£æå¼•ç”¨å¹¶æ ¼å¼åŒ–è¾“å‡º
        cited_refs = extract_cited_refs(final_text)
        print(format_citations(citation_infos, cited_refs))
        return

    # äº¤äº’å¼ REPL - é¢„çƒ­æ¨¡å‹
    warmup_models()
    print("RAG Agent äº¤äº’æ¨¡å¼ï¼šè¾“å…¥é—®é¢˜ï¼Œè¾“å…¥ /q é€€å‡ºã€‚")
    try:
        if args.enable_memory:
            # é»˜è®¤æµå¼è¾“å‡º + çŸ­æœŸè®°å¿†ï¼šä½¿ç”¨æŸ¥è¯¢æ”¹å†™é©±åŠ¨æ£€ç´¢ä¸ç”Ÿæˆ
            messages: List[dict] = []
            while True:
                try:
                    question = input("é—®> ").strip()
                except EOFError:
                    print("\nå·²é€€å‡ºã€‚")
                    break
                except KeyboardInterrupt:
                    print("\nå·²é€€å‡ºã€‚")
                    break

                if not question:
                    continue
                if question.lower() in {"/q", "q", ":q", "exit", "quit"}:
                    break

                # è¿½åŠ ç”¨æˆ·æ¶ˆæ¯ï¼Œå¹¶åŸºäºå†å²è¿›è¡ŒæŸ¥è¯¢æ”¹å†™
                messages.append({"role": "user", "content": question})
                rewritten = rewrite_question(messages)

                stream, citation_infos = agent.run_stream(rewritten)
                print("=== Final Answer ===")
                final_text_parts: List[str] = []
                for delta in stream:
                    final_text_parts.append(delta)
                    print(delta, end="", flush=True)
                final_text = "".join(final_text_parts)
                print()
                
                # è§£æå¼•ç”¨å¹¶æ ¼å¼åŒ–è¾“å‡º
                cited_refs = extract_cited_refs(final_text)
                print(format_citations(citation_infos, cited_refs))
                print()

                # å°†åŠ©æ‰‹æ¶ˆæ¯å†™å…¥è®°å¿†ï¼Œä¾›åç»­æ”¹å†™ä½¿ç”¨
                messages.append({"role": "assistant", "content": final_text})
        else:
            while True:
                try:
                    question = input("é—®> ").strip()
                except EOFError:
                    print("\nå·²é€€å‡ºã€‚")
                    break
                except KeyboardInterrupt:
                    print("\nå·²é€€å‡ºã€‚")
                    break

                if not question:
                    continue
                if question.lower() in {"/q", "q", ":q", "exit", "quit"}:
                    break

                stream, citation_infos = agent.run_stream(question)
                print("=== Final Answer ===")
                final_text_parts: List[str] = []
                for delta in stream:
                    final_text_parts.append(delta)
                    print(delta, end="", flush=True)
                final_text = "".join(final_text_parts)
                print()
                
                # è§£æå¼•ç”¨å¹¶æ ¼å¼åŒ–è¾“å‡º
                cited_refs = extract_cited_refs(final_text)
                print(format_citations(citation_infos, cited_refs))
                print()
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")


if __name__ == "__main__":
    main()