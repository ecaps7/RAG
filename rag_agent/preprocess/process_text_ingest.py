"""
æ–‡æœ¬æ•°æ®å¤„ç†ä¸å…¥åº“è„šæœ¬

åŠŸèƒ½ï¼š
1. è§£ææ ‡å‡†åŒ–æ–‡æœ¬æ•°æ®ï¼ˆåŒ…å«ç« èŠ‚ã€å®ä½“ã€æ¥æºç­‰å…ƒä¿¡æ¯ï¼‰
2. ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–è´¢åŠ¡æŒ‡æ ‡
3. ç”Ÿæˆå‘é‡ Embedding å¹¶å­˜å…¥ Milvus
4. æ„å»º BM25 å…³é”®è¯ç´¢å¼•
5. å°†æå–çš„æŒ‡æ ‡å­˜å…¥ SQLite

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python process_text_ingest.py --input-file outputs/CMB-2025-q1/CMB-2025-q1-text.json
"""

import sqlite3
import json
import jieba
import pickle
import ollama
import numpy as np
import os
import re
from pymilvus import MilvusClient
import argparse
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict, Tuple, Optional
from pydantic import BaseModel, Field

load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
SQL_DB_PATH = "database/financial_rag.db"
MILVUS_DB_PATH = "database/financial_vectors.db"
BM25_INDEX_PATH = "database/bm25_index.pkl"

# Embedding é…ç½®
OLLAMA_MODEL = "qwen3-embedding:4b"
EMBEDDING_DIM = 2560  # Qwen3-Embedding 4B çš„æ ‡å‡†ç»´åº¦

# LLM é…ç½®ï¼ˆç”¨äºç»“æ„åŒ–æŒ‡æ ‡æå–ï¼‰- è±†åŒ…æ¨¡å‹
LLM_API_BASE = os.getenv("LLM_API_BASE", "https://ark.cn-beijing.volces.com/api/v3")
LLM_API_KEY = os.getenv("LLM_API_KEY", os.getenv("ARK_API_KEY", ""))
LLM_MODEL = os.getenv("LLM_MODEL", "doubao-seed-1-6-251015")

# ================= Pydantic æ¨¡å‹å®šä¹‰ =================

class FinancialMetric(BaseModel):
    """å•ä¸ªè´¢åŠ¡æŒ‡æ ‡"""
    metric_name: str = Field(description="æŒ‡æ ‡åç§°ï¼Œå¦‚ï¼šè¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦ã€ä¸è‰¯è´·æ¬¾ç‡ç­‰")
    metric_value: float = Field(description="æŒ‡æ ‡æ•°å€¼ï¼Œçº¯æ•°å­—ï¼ˆå»é™¤åƒåˆ†ä½é€—å·ï¼‰")
    unit: str = Field(description="å•ä½ï¼Œå¦‚ï¼šç™¾ä¸‡å…ƒã€äº¿å…ƒã€å…ƒã€%ç­‰")

class MetricsExtractionResult(BaseModel):
    """è´¢åŠ¡æŒ‡æ ‡æå–ç»“æœ"""
    metrics: List[FinancialMetric] = Field(default_factory=list, description="æå–çš„è´¢åŠ¡æŒ‡æ ‡åˆ—è¡¨")

# ================= LLM æŒ‡æ ‡æå– Prompt =================

METRIC_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡æ•°æ®æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬æ®µè½ä¸­æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡å’Œæ•°æ®ã€‚

## æ–‡æ¡£ä¿¡æ¯
- æ¥æº: {entity}
- æŠ¥å‘ŠæœŸ: {report_period}
- ç« èŠ‚: {section}

## æ–‡æœ¬å†…å®¹
{content}

## æå–è¦æ±‚
1. åªæå–**å½“æœŸ**ï¼ˆæœ€æ–°æŠ¥å‘ŠæœŸï¼‰çš„æ˜ç¡®æ•°å€¼ï¼Œä¸è¦æå–åŒæ¯”/ç¯æ¯”æ•°æ®æˆ–ä¸Šå¹´æ•°æ®
2. æ¯ä¸ªæŒ‡æ ‡åŒ…å«: æŒ‡æ ‡åç§°ã€æ•°å€¼ã€å•ä½
3. æ•°å€¼è¯·è½¬æ¢ä¸ºçº¯æ•°å­—ï¼ˆå»é™¤åƒåˆ†ä½é€—å·ï¼Œè´Ÿæ•°ç”¨è´Ÿå·è¡¨ç¤ºï¼‰
4. å¸¸è§å•ä½ï¼šç™¾ä¸‡å…ƒã€äº¿å…ƒã€å…ƒã€%ã€ä¸ªç™¾åˆ†ç‚¹

## é‡ç‚¹æå–çš„æŒ‡æ ‡ç±»å‹
- æ”¶å…¥ç±»ï¼šè¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¯æ”¶å…¥ã€éåˆ©æ¯å‡€æ”¶å…¥ã€æ‰‹ç»­è´¹åŠä½£é‡‘å‡€æ”¶å…¥
- åˆ©æ¶¦ç±»ï¼šå‡€åˆ©æ¶¦ã€å½’å±äºè‚¡ä¸œçš„å‡€åˆ©æ¶¦
- æ¯è‚¡æŒ‡æ ‡ï¼šåŸºæœ¬æ¯è‚¡æ”¶ç›Šã€ç¨€é‡Šæ¯è‚¡æ”¶ç›Šã€æ¯è‚¡å‡€èµ„äº§
- èµ„äº§ç±»ï¼šèµ„äº§æ€»é¢ã€è´·æ¬¾å’Œå«æ¬¾æ€»é¢ã€å®¢æˆ·å­˜æ¬¾æ€»é¢
- è´Ÿå€ºç±»ï¼šè´Ÿå€ºæ€»é¢
- æƒç›Šç±»ï¼šè‚¡ä¸œæƒç›Š
- ç›ˆåˆ©èƒ½åŠ›ï¼šå‡€èµ„äº§æ”¶ç›Šç‡(ROE/ROAE)ã€æ€»èµ„äº§æ”¶ç›Šç‡(ROA/ROAA)ã€å‡€åˆ©å·®
- èµ„äº§è´¨é‡ï¼šä¸è‰¯è´·æ¬¾ç‡ã€ä¸è‰¯è´·æ¬¾ä½™é¢ã€æ‹¨å¤‡è¦†ç›–ç‡ã€è´·æ¬¾æ‹¨å¤‡ç‡
- èµ„æœ¬å……è¶³ï¼šæ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡ã€ä¸€çº§èµ„æœ¬å……è¶³ç‡ã€èµ„æœ¬å……è¶³ç‡

è¯·å°†æå–çš„æŒ‡æ ‡ä»¥ç»“æ„åŒ–æ ¼å¼è¿”å›ã€‚å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰å¯æå–çš„è´¢åŠ¡æŒ‡æ ‡ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
"""

# ================= å·¥å…·å‡½æ•° =================

llm_client = None

def get_llm_client():
    """è·å–æˆ–åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
    global llm_client
    if llm_client is None:
        if not LLM_API_KEY:
            raise ValueError("âŒ æœªè®¾ç½® LLM_API_KEY æˆ– DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        llm_client = OpenAI(
            api_key=LLM_API_KEY,
            base_url=LLM_API_BASE
        )
    return llm_client


def extract_metrics_from_text(text_item: Dict) -> List[Dict]:
    """
    ä½¿ç”¨ LLM ä»æ–‡æœ¬æ®µè½ä¸­æå–ç»“æ„åŒ–è´¢åŠ¡æŒ‡æ ‡ï¼ˆæ”¯æŒåŸç”Ÿç»“æ„åŒ–è¾“å‡ºï¼‰
    
    Args:
        text_item: æ–‡æœ¬æ•°æ®é¡¹ï¼ŒåŒ…å« content, entity, section ç­‰
        
    Returns:
        æå–çš„æŒ‡æ ‡åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« metric_name, metric_value, unit
    """
    try:
        content = text_item.get('content', '') or text_item.get('original_content', '')
        if not content or len(content.strip()) < 20:
            return []
        
        # è·å–å…ƒä¿¡æ¯
        entity = text_item.get('entity', 'æœªçŸ¥')
        section = text_item.get('section', '') or ' > '.join(text_item.get('section_path', []))
        source = text_item.get('source', '')
        
        # å°è¯•ä»æ¥æºæˆ–ç« èŠ‚ä¸­æå–æŠ¥å‘ŠæœŸä¿¡æ¯
        report_period = _extract_report_period(content, source, section)
        
        # å¿«é€Ÿè¿‡æ»¤ï¼šå¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰æ•°å­—ï¼Œè·³è¿‡
        if not re.search(r'\d+\.?\d*', content):
            return []
        
        # æ„å»º prompt
        prompt = METRIC_EXTRACTION_PROMPT.format(
            entity=entity,
            report_period=report_period,
            section=section,
            content=content
        )
        
        # è°ƒç”¨ LLMï¼ˆä½¿ç”¨åŸç”Ÿç»“æ„åŒ–è¾“å‡ºï¼‰
        client = get_llm_client()
        
        try:
            # å°è¯•ä½¿ç”¨åŸç”Ÿç»“æ„åŒ–è¾“å‡º
            completion = client.beta.chat.completions.parse(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                response_format=MetricsExtractionResult,
                temperature=0.1
            )
            
            result = completion.choices[0].message.parsed
            if result and result.metrics:
                metrics = [m.model_dump() for m in result.metrics]
            else:
                metrics = []
                
        except Exception as e:
            # é™çº§åˆ°ä¼ ç»Ÿæ–¹å¼
            print(f"   âš ï¸ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ–¹å¼: {e}")
            response = client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # è§£æ JSON
            json_match = re.search(r'\[[\s\S]*?\]', result_text)
            if json_match:
                metrics = json.loads(json_match.group())
            else:
                metrics = []
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        stock_code = _extract_stock_code(entity, source)
        company_name = _normalize_company_name(entity, stock_code)
        for m in metrics:
            m['stock_code'] = stock_code
            m['company_name'] = company_name
            m['report_period'] = _normalize_report_period(report_period)
            m['source_text_id'] = text_item.get('id', 'unknown')
        
        return metrics
        
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON è§£æå¤±è´¥ (æ–‡æœ¬å— {text_item.get('id', 'unknown')}): {e}")
        return []
    except Exception as e:
        print(f"âŒ LLM æå–å¤±è´¥ (æ–‡æœ¬å— {text_item.get('id', 'unknown')}): {e}")
        return []


def _extract_report_period(content: str, source: str, section: str) -> str:
    """ä»å†…å®¹ã€æ¥æºæˆ–ç« èŠ‚ä¸­æå–æŠ¥å‘ŠæœŸ"""
    # ä¼˜å…ˆä» source ä¸­æå–ï¼ˆå¦‚ "CMB-2025-q1"ï¼‰
    if source:
        match = re.search(r'(\d{4})[-_](q[1-4]|Q[1-4]|h[1-2]|H[1-2]|year|annual)', source, re.IGNORECASE)
        if match:
            year = match.group(1)
            period = match.group(2).upper()
            if period.startswith('Q'):
                return f"{year}å¹´ç¬¬{['ä¸€','äºŒ','ä¸‰','å››'][int(period[1])-1]}å­£åº¦"
            elif period.startswith('H'):
                return f"{year}å¹´åŠå¹´åº¦"
            else:
                return f"{year}å¹´å¹´åº¦"
    
    # ä»ç« èŠ‚æ ‡é¢˜ä¸­æå–
    period_match = re.search(r'(äºŒã€‡\d{2}|20\d{2})å¹´(ç¬¬[ä¸€äºŒä¸‰å››]å­£åº¦|åŠå¹´åº¦|å¹´åº¦)', section)
    if period_match:
        return period_match.group(0)
    
    # ä»å†…å®¹ä¸­æå–
    period_match = re.search(r'(20\d{2})å¹´(ç¬¬[ä¸€äºŒä¸‰å››]å­£åº¦|åŠå¹´åº¦|å¹´åº¦|[1-3][-~]\d+æœˆ)', content)
    if period_match:
        return period_match.group(0)
    
    return "æœªçŸ¥"


def _extract_stock_code(entity: str, source: str) -> str:
    """ä»å®ä½“åç§°æˆ–æ¥æºä¸­æå–è‚¡ç¥¨ä»£ç """
    # ä» source ä¸­æå–ï¼ˆå¦‚ "CMB" -> "600036.SH"ï¼‰
    code_map = {
        'CMB': '600036.SH',
        'æ‹›å•†é“¶è¡Œ': '600036.SH',
        'æ‹›è¡Œ': '600036.SH',
        'CITIC': '601998.SH',
        'ä¸­ä¿¡é“¶è¡Œ': '601998.SH',
        'ä¸­ä¿¡': '601998.SH'
    }
    
    for key, code in code_map.items():
        if key in (source or '') or key in (entity or ''):
            return code
    
    # ä» entity ä¸­æå–æ•°å­—ä»£ç å¹¶æ·»åŠ  .SH åç¼€
    code_match = re.search(r'\b(\d{6})\b', entity or '')
    if code_match:
        return f"{code_match.group(1)}.SH"
    
    # å¦‚æœå®ä½“åç§°ä¸­å·²åŒ…å«å®Œæ•´ä»£ç ï¼ˆå¦‚ "601998.SH"ï¼‰
    full_code_match = re.search(r'\b(\d{6}\.(SH|HK))\b', entity or '', re.IGNORECASE)
    if full_code_match:
        return full_code_match.group(1).upper()
    
    return entity or 'æœªçŸ¥'


def _normalize_company_name(entity: str, stock_code: str) -> str:
    """
    æ ‡å‡†åŒ–å…¬å¸åç§°ï¼Œä½¿ç”¨ç»Ÿä¸€çš„ç®€ç§°
    
    Args:
        entity: å®ä½“åç§°
        stock_code: è‚¡ç¥¨ä»£ç 
        
    Returns:
        æ ‡å‡†åŒ–åçš„å…¬å¸åç§°
    """
    # æ ¹æ®è‚¡ç¥¨ä»£ç æ˜ å°„
    code_to_name = {
        '600036.SH': 'æ‹›å•†é“¶è¡Œ',
        '601998.SH': 'ä¸­ä¿¡é“¶è¡Œ'
    }
    
    if stock_code in code_to_name:
        return code_to_name[stock_code]
    
    # æ ¹æ®å®ä½“åç§°æ˜ å°„
    name_map = {
        'CMB': 'æ‹›å•†é“¶è¡Œ',
        'æ‹›å•†é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸': 'æ‹›å•†é“¶è¡Œ',
        'æ‹›è¡Œ': 'æ‹›å•†é“¶è¡Œ',
        'CITIC': 'ä¸­ä¿¡é“¶è¡Œ',
        'ä¸­ä¿¡é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸': 'ä¸­ä¿¡é“¶è¡Œ',
        'ä¸­ä¿¡': 'ä¸­ä¿¡é“¶è¡Œ'
    }
    
    for key, name in name_map.items():
        if key in (entity or ''):
            return name
    
    return entity or 'æœªçŸ¥'


def _normalize_report_period(period: str) -> str:
    """
    æ ‡å‡†åŒ–æŠ¥å‘ŠæœŸæ ¼å¼
    ä¾‹å¦‚: "2025å¹´ç¬¬ä¸€å­£åº¦" -> "2025-Q1"
    """
    period = period.strip()
    
    # åŒ¹é… "2025å¹´ç¬¬ä¸€å­£åº¦" æˆ– "äºŒã€‡äºŒäº”å¹´ç¬¬ä¸€å­£åº¦"
    q_match = re.search(r'(äºŒã€‡\d{2}|20\d{2})å¹´ç¬¬([ä¸€äºŒä¸‰å››])å­£åº¦', period)
    if q_match:
        year_str = q_match.group(1)
        # è½¬æ¢ä¸­æ–‡å¹´ä»½
        if year_str.startswith('äºŒã€‡'):
            year = '20' + year_str[2:]
        else:
            year = year_str
        q_map = {'ä¸€': 'Q1', 'äºŒ': 'Q2', 'ä¸‰': 'Q3', 'å››': 'Q4'}
        quarter = q_map.get(q_match.group(2), 'Q1')
        return f"{year}-{quarter}"
    
    # åŒ¹é… "2025å¹´1-3æœˆ"
    month_match = re.search(r'(\d{4})å¹´(\d+)[-~](\d+)æœˆ', period)
    if month_match:
        year = month_match.group(1)
        end_month = int(month_match.group(3))
        if end_month == 3:
            return f"{year}-Q1"
        elif end_month == 6:
            return f"{year}-Q2"
        elif end_month == 9:
            return f"{year}-Q3"
        elif end_month == 12:
            return f"{year}-Q4"
    
    # åŒ¹é… "2025å¹´åŠå¹´åº¦"
    h_match = re.search(r'(\d{4})å¹´(åŠå¹´åº¦|ä¸ŠåŠå¹´)', period)
    if h_match:
        year = h_match.group(1)
        return f"{year}-H1"
    
    # åŒ¹é… "2025å¹´å¹´åº¦"
    y_match = re.search(r'(\d{4})å¹´(å¹´åº¦|åº¦)', period)
    if y_match:
        year = y_match.group(1)
        return f"{year}-FY"
    
    return period


def get_embedding(text: str) -> List[float]:
    """è°ƒç”¨ Ollama ç”Ÿæˆå‘é‡"""
    try:
        text = text.replace("\n", " ").strip()
        if not text:
            return np.zeros(EMBEDDING_DIM).tolist()
            
        response = ollama.embeddings(model=OLLAMA_MODEL, prompt=text)
        embedding = response.get('embedding')
        
        if not embedding or len(embedding) != EMBEDDING_DIM:
            print(f"âš ï¸ è­¦å‘Š: å‘é‡ç»´åº¦å¼‚å¸¸æˆ–ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
            return np.zeros(EMBEDDING_DIM).tolist()
            
        return embedding
    except Exception as e:
        print(f"âŒ Embedding è°ƒç”¨å¤±è´¥: {e}")
        return np.zeros(EMBEDDING_DIM).tolist()


def init_sqlite() -> sqlite3.Connection:
    """åˆå§‹åŒ– SQLite ç»“æ„åŒ–æŒ‡æ ‡åº“"""
    db_dir = os.path.dirname(SQL_DB_PATH)
    if db_dir and not os.path.exists(db_dir):
        os.makedirs(db_dir)
        print(f"   ğŸ“ åˆ›å»ºç›®å½•: {db_dir}")
    
    conn = sqlite3.connect(SQL_DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS financial_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            stock_code TEXT,
            company_name TEXT,
            report_period TEXT,
            metric_name TEXT,
            metric_value REAL,
            unit TEXT,
            source_table_id TEXT,
            UNIQUE(stock_code, report_period, metric_name)
        )
    ''')
    conn.commit()
    return conn


def init_milvus() -> Tuple[MilvusClient, str]:
    """åˆå§‹åŒ– Milvus å‘é‡åº“"""
    db_dir = os.path.dirname(MILVUS_DB_PATH)
    if db_dir and not os.path.exists(db_dir):
        os.makedirs(db_dir)
        print(f"   ğŸ“ åˆ›å»ºç›®å½•: {db_dir}")
    
    client = MilvusClient(uri=MILVUS_DB_PATH)
    collection_name = "financial_chunks"
    
    if not client.has_collection(collection_name):
        client.create_collection(
            collection_name=collection_name,
            dimension=EMBEDDING_DIM,
            metric_type="COSINE",
            auto_id=True
        )
    
    return client, collection_name


# ================= ä¸»æµç¨‹ =================

def main():
    parser = argparse.ArgumentParser(
        description="process_text_ingest.py: å¤„ç†æ ‡å‡†åŒ–æ–‡æœ¬æ•°æ®å¹¶å…¥åº“"
    )
    parser.add_argument("--input-file", type=str, required=True,
                        help="è¾“å…¥æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    args = parser.parse_args()
    
    input_file = args.input_file
    
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {input_file}")
        return
    
    # 2. åˆå§‹åŒ–æ•°æ®åº“
    print("ğŸ› ï¸ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“...")
    sql_conn = init_sqlite()
    sql_cursor = sql_conn.cursor()
    milvus_client, collection_name = init_milvus()
    
    # 3. åŠ è½½æ–‡æœ¬æ•°æ®
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ–‡æœ¬æ–‡ä»¶: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        text_data = json.load(f)
    
    if not isinstance(text_data, list):
        print("âŒ é”™è¯¯ï¼šæœŸæœ› JSON æ ¼å¼ä¸ºåˆ—è¡¨")
        return
    
    print(f"   âœ“ åŠ è½½äº† {len(text_data)} ä¸ªæ–‡æœ¬å—")
    
    # ---------------------------------------------------------
    # A. SQL Layer: ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æŒ‡æ ‡
    # ---------------------------------------------------------
    print("ğŸ“Š æ­£åœ¨ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æŒ‡æ ‡...")
    
    all_metrics = []
    texts_processed = 0
    
    for item in text_data:
        metrics = extract_metrics_from_text(item)
        if metrics:
            all_metrics.extend(metrics)
            texts_processed += 1
            print(f"   âœ“ æ–‡æœ¬å— {item.get('id', 'unknown')}: æå–äº† {len(metrics)} ä¸ªæŒ‡æ ‡")
    
    # å»é‡ï¼šåŒä¸€å…¬å¸ã€åŒä¸€æŠ¥å‘ŠæœŸã€åŒä¸€æŒ‡æ ‡åç§°åªä¿ç•™ä¸€æ¡
    # å¦‚æœæœ‰å¤šä¸ªæ¥æºæå–äº†ç›¸åŒæŒ‡æ ‡ï¼Œä¼˜å…ˆä¿ç•™ç¬¬ä¸€ä¸ª
    seen = set()
    unique_metrics = []
    for m in all_metrics:
        key = (m['stock_code'], m['report_period'], m['metric_name'])
        if key not in seen:
            seen.add(key)
            unique_metrics.append(m)
        else:
            # è®°å½•è¢«å»é‡çš„æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            pass  # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ—¥å¿—
    
    # å†™å…¥ SQLite
    if unique_metrics:
        sql_records = [
            (
                m['stock_code'],
                m['company_name'],
                m['report_period'],
                m['metric_name'],
                m['metric_value'],
                m['unit'],
                m.get('source_text_id', 'unknown')
            )
            for m in unique_metrics
        ]
        
        sql_cursor.executemany('''
            INSERT OR REPLACE INTO financial_metrics 
            (stock_code, company_name, report_period, metric_name, metric_value, unit, source_table_id)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', sql_records)
        sql_conn.commit()
        print(f"   --> ä» {texts_processed} ä¸ªæ–‡æœ¬å—ä¸­æå–å¹¶å­˜å…¥ {len(unique_metrics)} æ¡æŒ‡æ ‡ï¼ˆå»é‡åï¼‰")
    else:
        print("   --> æœªæå–åˆ°ä»»ä½•æŒ‡æ ‡")
    
    # ---------------------------------------------------------
    # B. Vector + Keyword Layer: å¤„ç†å‘é‡å’Œ BM25
    # ---------------------------------------------------------
    print("ğŸ§  æ­£åœ¨å¤„ç†å‘é‡ Embedding å’Œ BM25 åˆ†è¯...")
    
    milvus_data = []
    bm25_corpus = []
    doc_map = []
    
    for item in text_data:
        content = item.get('content', '') or item.get('original_content', '')
        if not content or len(content.strip()) < 10:
            continue
        
        # 1. ç”Ÿæˆå‘é‡
        vec = get_embedding(content)
        
        # 2. å‡†å¤‡ Milvus æ•°æ®
        metadata_dict = {
            "source_id": str(item.get('id', 'unknown')),
            "type": "text",
            "page": str(item.get('page', 0)),
            "raw_data": content,
            "section": item.get('section', '') or ' > '.join(item.get('section_path', [])),
            "company_name": item.get('entity', ''),
            "stock_code": _extract_stock_code(item.get('entity', ''), item.get('source', '')),
            "report_period": _extract_report_period(content, item.get('source', ''), 
                                                     item.get('section', '')),
            "source": item.get('source', '')
        }
        
        entry = {
            "vector": vec,
            "text": content,
            "subject": "text_chunk",
            "metadata": json.dumps(metadata_dict, ensure_ascii=False)
        }
        milvus_data.append(entry)
        
        # 3. å‡†å¤‡ BM25 æ•°æ®
        tokens = list(jieba.cut_for_search(content))
        bm25_corpus.append(tokens)
        doc_map.append(entry)
    
    # ---------------------------------------------------------
    # C. å†™å…¥å­˜å‚¨
    # ---------------------------------------------------------
    
    # 1. å†™å…¥ Milvusï¼ˆå»é‡ï¼‰
    if milvus_data:
        print(f"ğŸ’¾ æ­£åœ¨å†™å…¥ Milvus ({len(milvus_data)} æ¡æ•°æ®)...")
        
        # æ£€æŸ¥å·²å­˜åœ¨çš„æ•°æ® (ä½¿ç”¨ company_name + source_id ç»„åˆå»é‡)
        existing_keys = set()
        try:
            batch_size = 16384
            offset = 0
            while True:
                batch_data = milvus_client.query(
                    collection_name=collection_name,
                    filter="",
                    output_fields=["metadata"],
                    limit=batch_size,
                    offset=offset
                )
                if not batch_data:
                    break
                    
                for item in batch_data:
                    try:
                        metadata = json.loads(item.get('metadata', '{}'))
                        company = metadata.get('company_name', '')
                        source_id = metadata.get('source_id', '')
                        # ä½¿ç”¨å…¬å¸å+source_idç»„åˆä½œä¸ºå”¯ä¸€é”®
                        existing_keys.add(f"{company}::{source_id}")
                    except:
                        pass
                
                if len(batch_data) < batch_size:
                    break
                offset += batch_size
            
            if existing_keys:
                print(f"   â„¹ï¸ å‘ç° {len(existing_keys)} æ¡å·²å­˜åœ¨çš„æ•°æ®")
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•æ£€æŸ¥å·²å­˜åœ¨æ•°æ®: {e}")
        
        # è¿‡æ»¤é‡å¤æ•°æ® (åŸºäº company_name + source_id ç»„åˆ)
        new_data = []
        skipped = 0
        for entry in milvus_data:
            metadata = json.loads(entry['metadata'])
            company = metadata.get('company_name', '')
            source_id = metadata.get('source_id', '')
            key = f"{company}::{source_id}"
            if key not in existing_keys:
                new_data.append(entry)
            else:
                skipped += 1
        
        if new_data:
            res = milvus_client.insert(collection_name=collection_name, data=new_data)
            print(f"   --> æˆåŠŸå†™å…¥ {res['insert_count']} æ¡æ–°å‘é‡ï¼Œè·³è¿‡ {skipped} æ¡é‡å¤æ•°æ®")
        else:
            print(f"   --> æ‰€æœ‰æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ {skipped} æ¡é‡å¤æ•°æ®")
    
    # 2. æ„å»ºå¹¶ä¿å­˜ BM25ï¼ˆæ”¯æŒè¿½åŠ ï¼‰
    existing_doc_map = []
    if os.path.exists(BM25_INDEX_PATH):
        try:
            with open(BM25_INDEX_PATH, 'rb') as f:
                _, existing_doc_map = pickle.load(f)
            print(f"   â„¹ï¸ å‘ç°å·²æœ‰ BM25 ç´¢å¼•ï¼ŒåŒ…å« {len(existing_doc_map)} æ¡æ–‡æ¡£")
        except Exception as e:
            print(f"   âš ï¸ è¯»å–å·²æœ‰ BM25 ç´¢å¼•å¤±è´¥: {e}")
    
    if bm25_corpus or existing_doc_map:
        print("ğŸ“‘ æ­£åœ¨æ„å»º/æ›´æ–° BM25 ç´¢å¼•...")
        from rank_bm25 import BM25Okapi
        
        # æ£€æŸ¥é‡å¤ (ä½¿ç”¨ company_name + source_id ç»„åˆ)
        existing_keys_bm25 = set()
        for entry in existing_doc_map:
            try:
                metadata = json.loads(entry.get('metadata', '{}'))
                company = metadata.get('company_name', '')
                source_id = metadata.get('source_id', '')
                existing_keys_bm25.add(f"{company}::{source_id}")
            except:
                pass
        
        # è¿‡æ»¤é‡å¤æ•°æ® (åŸºäº company_name + source_id ç»„åˆ)
        filtered_corpus = []
        filtered_doc_map = []
        skipped_bm25 = 0
        
        for i, entry in enumerate(doc_map):
            try:
                metadata = json.loads(entry.get('metadata', '{}'))
                company = metadata.get('company_name', '')
                source_id = metadata.get('source_id', '')
                key = f"{company}::{source_id}"
                if key not in existing_keys_bm25:
                    filtered_corpus.append(bm25_corpus[i])
                    filtered_doc_map.append(entry)
                else:
                    skipped_bm25 += 1
            except:
                filtered_corpus.append(bm25_corpus[i])
                filtered_doc_map.append(entry)
        
        if skipped_bm25 > 0:
            print(f"   â„¹ï¸ BM25 è·³è¿‡ {skipped_bm25} æ¡é‡å¤æ•°æ®")
        
        # å‡†å¤‡å…¨é‡è¯­æ–™
        full_corpus = []
        
        # é‡æ–°åˆ†è¯æ—§æ•°æ®
        if existing_doc_map:
            print(f"   ...æ­£åœ¨é‡æ–°åˆ†è¯æ—§æ•°æ® ({len(existing_doc_map)} æ¡)...")
            for entry in existing_doc_map:
                full_corpus.append(list(jieba.cut_for_search(entry['text'])))
        
        full_corpus.extend(filtered_corpus)
        full_doc_map = existing_doc_map + filtered_doc_map
        
        bm25 = BM25Okapi(full_corpus)
        
        # ä¿å­˜ç´¢å¼•
        bm25_dir = os.path.dirname(BM25_INDEX_PATH)
        if bm25_dir and not os.path.exists(bm25_dir):
            os.makedirs(bm25_dir)
            print(f"   ğŸ“ åˆ›å»ºç›®å½•: {bm25_dir}")
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ BM25 ç´¢å¼•åˆ° {BM25_INDEX_PATH}...")
        with open(BM25_INDEX_PATH, 'wb') as f:
            pickle.dump((bm25, full_doc_map), f)
        print(f"   --> BM25 ç´¢å¼•ä¿å­˜å®Œæˆï¼ˆå…± {len(full_doc_map)} æ¡æ–‡æ¡£ï¼‰")
    
    # æ”¶å°¾
    sql_conn.close()
    print("\nğŸ‰ æ–‡æœ¬æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"   ğŸ“Š æå–æŒ‡æ ‡: {len(unique_metrics)} æ¡")
    print(f"   ğŸ§  å‘é‡æ•°æ®: {len(new_data) if milvus_data else 0} æ¡")
    print(f"   ğŸ“‘ BM25 ç´¢å¼•: {len(full_doc_map) if bm25_corpus or existing_doc_map else 0} æ¡")


if __name__ == "__main__":
    main()
