"""
è¡¨æ ¼æ•°æ®å¤„ç†ä¸å…¥åº“è„šæœ¬

åŠŸèƒ½ï¼š
1. è§£ææ ‡å‡†åŒ–è¡¨æ ¼æ•°æ®ï¼ˆåŒ…å«å…¨å±€æ–‡æ¡£ä¿¡æ¯å’Œè¡¨æ ¼è¯¦æƒ…ï¼‰
2. ä½¿ç”¨ LLM ä» HTML è¡¨æ ¼ä¸­æå–ç»“æ„åŒ–è´¢åŠ¡æŒ‡æ ‡
3. ç”Ÿæˆå‘é‡ Embedding å¹¶å­˜å…¥ Milvus
4. æ„å»º BM25 å…³é”®è¯ç´¢å¼•
5. å°†æå–çš„æŒ‡æ ‡å­˜å…¥ SQLite

æ•°æ®æ ¼å¼ï¼š
{
  "document": {
    "source": "CMB-2025-q1",
    "company": "æ‹›å•†é“¶è¡Œ",
    "company_full": "æ‹›å•†é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸",
    "stock_code": "600036.SH / 03968.HK",
    "report_period": "2025å¹´ç¬¬ä¸€å­£åº¦",
    "report_type": "å­£æŠ¥",
    "fiscal_year": "2025"
  },
  "tables": [
    {
      "id": "TABLE_1",
      "summary": "è¡¨æ ¼æ‘˜è¦...",
      "page": 2,
      "section": ["ç« èŠ‚1", "ç« èŠ‚2"],
      "raw_html": "<table>...</table>",
      "context": {"before": "...", "after": "..."},
      "bbox": [x1, y1, x2, y2]
    }
  ]
}

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python process_table_ingest.py --input-file outputs/CMB-2025-q1/CMB-2025-q1-table.json
"""

import sqlite3
import json
import jieba
import pickle
import ollama
import numpy as np
import os
import re
from pymilvus import MilvusClient
import argparse
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict, Tuple, Optional
from pydantic import BaseModel, Field

load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
SQL_DB_PATH = "database/financial_rag.db"
MILVUS_DB_PATH = "database/financial_vectors.db"
BM25_INDEX_PATH = "database/bm25_index.pkl"

# Embedding é…ç½®
OLLAMA_MODEL = "qwen3-embedding:4b"
EMBEDDING_DIM = 2560  # Qwen3-Embedding 4B çš„æ ‡å‡†ç»´åº¦

# LLM é…ç½®ï¼ˆç”¨äºç»“æ„åŒ–æŒ‡æ ‡æå–ï¼‰- è±†åŒ…æ¨¡å‹
LLM_API_BASE = os.getenv("LLM_API_BASE", "https://ark.cn-beijing.volces.com/api/v3")
LLM_API_KEY = os.getenv("LLM_API_KEY", os.getenv("ARK_API_KEY", ""))
LLM_MODEL = os.getenv("LLM_MODEL", "doubao-seed-1-6-251015")

# æ˜ç¡®æ’é™¤çš„è¡¨æ ¼ç±»å‹ï¼ˆä¸åŒ…å«è´¢åŠ¡æŒ‡æ ‡ï¼‰
EXCLUDE_TABLE_KEYWORDS = [
    "è‚¡ä¸œæƒ…å†µ", "è‚¡ä¸œä¿¡æ¯", "è‘£äº‹", "ç›‘äº‹", "é«˜ç®¡",
    "å…¬å¸æ²»ç†", "å…³è”äº¤æ˜“", "é‡å¤§äº‹é¡¹", "å…¬å¸åŸºæœ¬æƒ…å†µ"
]

# ================= Pydantic æ¨¡å‹å®šä¹‰ =================

class FinancialMetric(BaseModel):
    """å•ä¸ªè´¢åŠ¡æŒ‡æ ‡"""
    metric_name: str = Field(description="æŒ‡æ ‡åç§°ï¼Œå¦‚ï¼šè¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦ã€ä¸è‰¯è´·æ¬¾ç‡ç­‰")
    metric_value: float = Field(description="æŒ‡æ ‡æ•°å€¼ï¼Œçº¯æ•°å­—ï¼ˆå»é™¤åƒåˆ†ä½é€—å·ï¼‰")
    unit: str = Field(description="å•ä½ï¼Œå¦‚ï¼šç™¾ä¸‡å…ƒã€äº¿å…ƒã€å…ƒã€%ç­‰")

class MetricsExtractionResult(BaseModel):
    """è´¢åŠ¡æŒ‡æ ‡æå–ç»“æœ"""
    metrics: List[FinancialMetric] = Field(default_factory=list, description="æå–çš„è´¢åŠ¡æŒ‡æ ‡åˆ—è¡¨")

# ================= LLM æŒ‡æ ‡æå– Prompt =================

METRIC_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„è·¨é“¶è¡Œå¤šå‘¨æœŸè´¢åŠ¡æ•°æ®æå–åŠ©æ‰‹ï¼Œèƒ½é€‚é…ä¸åŒé“¶è¡Œã€ä¸åŒæŠ¥å‘ŠæœŸé—´ï¼ˆå­£åº¦/åŠå¹´åº¦/å¹´åº¦ï¼‰çš„è´¢åŠ¡æŠ¥å‘Šè¡¨æ ¼ï¼Œç²¾å‡†æå–æ ‡å‡†åŒ–æŒ‡æ ‡ã€‚è¯·ä»ä»¥ä¸‹ HTML è¡¨æ ¼ä¸­æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼š

## æŠ¥å‘ŠåŸºç¡€ä¿¡æ¯
- å…¬å¸åç§°: {company_name}
- è‚¡ç¥¨ä»£ç : {stock_code}
- æŠ¥å‘ŠæœŸæ—¶é—´èŒƒå›´: {report_period}ï¼ˆä¾‹ï¼š2025å¹´1-3æœˆã€2024å¹´1-12æœˆï¼‰
- è¡¨æ ¼æ¥æº: {table_id}

## HTML è¡¨æ ¼å†…å®¹
```html
{raw_html}
```

## æ ¸å¿ƒæå–è§„åˆ™
1. ä»…æå–**å½“æœŸæ•°æ®**ï¼šæŠ¥å‘ŠæœŸæ—¶é—´èŒƒå›´å†…çš„å‘ç”Ÿé¢ï¼ˆå¦‚æ”¶å…¥ã€åˆ©æ¶¦ï¼‰ã€æŠ¥å‘ŠæœŸæœ«çš„æ—¶ç‚¹é¢ï¼ˆå¦‚èµ„äº§ã€è´Ÿå€ºï¼‰ï¼Œä¸æå–åŒæ¯”/ç¯æ¯”å¢å‡ç‡ã€ä¸Šå¹´åŒæœŸ/ä¸Šå¹´æœ«æ•°æ®åŠå¢å‡é¢ï¼›
2. æŒ‡æ ‡ä¸‰è¦ç´ å®Œæ•´ï¼šæ¯ä¸ªæŒ‡æ ‡å¿…é¡»åŒ…å«ã€Œæ ‡å‡†åŒ–æŒ‡æ ‡åç§°ã€ã€Œçº¯æ•°å­—æ•°å€¼ã€ã€Œæ˜ç¡®å•ä½ã€ï¼Œç¼ºä¸€ä¸å¯ï¼›
3. æ•°å€¼æ ¼å¼ç»Ÿä¸€ï¼šå»é™¤åƒåˆ†ä½é€—å·ï¼Œè´Ÿæ•°ä»¥è´Ÿå·ï¼ˆ-ï¼‰è¡¨ç¤ºï¼Œä¿ç•™åŸå§‹ç²¾åº¦ï¼ˆæ— å°æ•°æŒ‰æ•´æ•°ã€æœ‰å°æ•°æŒ‰åŸä½æ•°ï¼‰ï¼›
4. å•ä½è§„èŒƒï¼šä¼˜å…ˆä½¿ç”¨è¡¨æ ¼æ ‡æ³¨å•ä½ï¼ˆå¦‚æ— æ ‡æ³¨ï¼Œå‚è€ƒå¸¸è§å•ä½ï¼šç™¾ä¸‡å…ƒã€äº¿å…ƒã€å…ƒã€%ã€ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œé¿å…å•ä½æ··æ·†ï¼ˆä¾‹ï¼šæ˜ç¡®åŒºåˆ†ã€Œ%ã€ä¸ã€Œä¸ªç™¾åˆ†ç‚¹ã€ï¼‰ï¼›
5. æŠ¥è¡¨ä¼˜å…ˆçº§ï¼šä¼˜å…ˆæå–åˆå¹¶æŠ¥è¡¨æ•°æ®ï¼Œè‹¥è¡¨æ ¼æ— åˆå¹¶æŠ¥è¡¨æ ‡è¯†æˆ–ä»…ä¸ºæ¯å…¬å¸æŠ¥è¡¨ï¼Œéœ€åœ¨æŒ‡æ ‡åç§°åæ ‡æ³¨ã€Œï¼ˆæ¯å…¬å¸ï¼‰ã€ï¼›
6. æŒ‡æ ‡åç§°æ ‡å‡†åŒ–ï¼šç»Ÿä¸€æŒ‡æ ‡å‘½åï¼ˆä¾‹ï¼šã€Œå½’å±äºæ¯å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦ã€ã€Œæ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡ï¼ˆé«˜çº§æ³•ï¼‰ã€ï¼Œé¿å…ã€Œå½’å±äºè‚¡ä¸œå‡€åˆ©æ¶¦ã€ã€Œé«˜çº§æ³•ä¸‹æ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡ã€ç­‰ä¸ç»Ÿä¸€è¡¨è¿°ï¼‰ï¼›
7. é‡å¤æŒ‡æ ‡å¤„ç†ï¼šåŒä¸€æŒ‡æ ‡åœ¨è¡¨æ ¼ä¸­å¤šæ¬¡å‡ºç°æ—¶ï¼Œå–åˆå¹¶æŠ¥è¡¨æ•°æ®ï¼ˆæ— åˆå¹¶æŠ¥è¡¨åˆ™å–æœ€æ–°å‡ºç°çš„æœ‰æ•ˆå€¼ï¼‰ï¼Œä¸é‡å¤æå–ã€‚

## é‡ç‚¹æå–æŒ‡æ ‡ç±»å‹åŠæ ‡å‡†åŒ–åç§°
### 1. æ”¶å…¥ç±»
- è¥ä¸šæ”¶å…¥
- å‡€åˆ©æ¯æ”¶å…¥
- éåˆ©æ¯å‡€æ”¶å…¥
- æ‰‹ç»­è´¹åŠä½£é‡‘å‡€æ”¶å…¥
- æŠ•èµ„æ”¶ç›Š
- æ±‡å…‘å‡€æ”¶ç›Šï¼ˆæˆ–ï¼šæ±‡å…‘æ”¶ç›Šï¼Œæ ¹æ®è¡¨æ ¼è¡¨è¿°é€‚é…ï¼‰
- å…¶ä»–å‡€æ”¶å…¥å°è®¡

### 2. åˆ©æ¶¦ç±»
- å‡€åˆ©æ¶¦
- å½’å±äºæ¯å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦ï¼ˆæˆ–ï¼šå½’å±äºè‚¡ä¸œçš„å‡€åˆ©æ¶¦ï¼Œæ ¹æ®è¡¨æ ¼è¡¨è¿°é€‚é…ï¼‰
- æ‰£é™¤éç»å¸¸æ€§æŸç›Šåå½’å±äºæ¯å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦ï¼ˆæˆ–ï¼šæ‰£éå‡€åˆ©æ¶¦ï¼Œæ ¹æ®è¡¨æ ¼è¡¨è¿°é€‚é…ï¼‰

### 3. æ¯è‚¡æŒ‡æ ‡
- åŸºæœ¬æ¯è‚¡æ”¶ç›Š
- ç¨€é‡Šæ¯è‚¡æ”¶ç›Š
- æ¯è‚¡å‡€èµ„äº§ï¼ˆæˆ–ï¼šå½’å±äºæ™®é€šè‚¡è‚¡ä¸œçš„æ¯è‚¡å‡€èµ„äº§ï¼Œæ ¹æ®è¡¨æ ¼è¡¨è¿°é€‚é…ï¼‰

### 4. èµ„äº§ç±»
- èµ„äº§æ€»é¢
- è´·æ¬¾å’Œå«æ¬¾æ€»é¢
- å®¢æˆ·å­˜æ¬¾æ€»é¢
- é‡‘èæŠ•èµ„ä½™é¢
- ä¸è‰¯è´·æ¬¾ä½™é¢
- å…³æ³¨è´·æ¬¾ä½™é¢
- é€¾æœŸè´·æ¬¾ä½™é¢

### 5. è´Ÿå€ºç±»
- è´Ÿå€ºæ€»é¢

### 6. æƒç›Šç±»
- è‚¡ä¸œæƒç›Šåˆè®¡ï¼ˆæˆ–ï¼šè‚¡ä¸œæƒç›Šï¼‰
- å½’å±äºæ¯å…¬å¸è‚¡ä¸œçš„æƒç›Šï¼ˆæˆ–ï¼šå½’å±äºè‚¡ä¸œæƒç›Šï¼Œæ ¹æ®è¡¨æ ¼è¡¨è¿°é€‚é…ï¼‰

### 7. ç›ˆåˆ©èƒ½åŠ›
- å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROE/ROAEï¼‰ï¼ˆå¹´åŒ–æ•°æ®ä¼˜å…ˆï¼Œæ ‡æ³¨ã€Œï¼ˆå¹´åŒ–ï¼‰ã€ï¼‰
- æ‰£é™¤éç»å¸¸æ€§æŸç›Šåå‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROE/ROAEï¼‰ï¼ˆå¹´åŒ–æ•°æ®ä¼˜å…ˆï¼Œæ ‡æ³¨ã€Œï¼ˆå¹´åŒ–ï¼‰ã€ï¼‰
- æ€»èµ„äº§æ”¶ç›Šç‡ï¼ˆROA/ROAAï¼‰ï¼ˆå¹´åŒ–æ•°æ®ä¼˜å…ˆï¼Œæ ‡æ³¨ã€Œï¼ˆå¹´åŒ–ï¼‰ã€ï¼‰
- å‡€åˆ©å·®
- å‡€åˆ©æ¯æ”¶ç›Šç‡

### 8. èµ„äº§è´¨é‡
- ä¸è‰¯è´·æ¬¾ç‡
- å…³æ³¨è´·æ¬¾ç‡
- é€¾æœŸè´·æ¬¾ç‡
- æ‹¨å¤‡è¦†ç›–ç‡
- è´·æ¬¾æ‹¨å¤‡ç‡

### 9. èµ„æœ¬å……è¶³
- æ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡ï¼ˆé«˜çº§æ³•ï¼‰ï¼ˆå¦‚æœ‰ï¼‰
- æ ¸å¿ƒä¸€çº§èµ„æœ¬å……è¶³ç‡ï¼ˆæƒé‡æ³•ï¼‰ï¼ˆå¦‚æœ‰ï¼‰
- ä¸€çº§èµ„æœ¬å……è¶³ç‡ï¼ˆé«˜çº§æ³•ï¼‰ï¼ˆå¦‚æœ‰ï¼‰
- ä¸€çº§èµ„æœ¬å……è¶³ç‡ï¼ˆæƒé‡æ³•ï¼‰ï¼ˆå¦‚æœ‰ï¼‰
- èµ„æœ¬å……è¶³ç‡ï¼ˆé«˜çº§æ³•ï¼‰ï¼ˆå¦‚æœ‰ï¼‰
- èµ„æœ¬å……è¶³ç‡ï¼ˆæƒé‡æ³•ï¼‰ï¼ˆå¦‚æœ‰ï¼‰

### 10. ç°é‡‘æµ
- ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢

è¯·å°†æå–çš„æŒ‡æ ‡ä»¥ç»“æ„åŒ–æ ¼å¼è¿”å›ã€‚å¦‚æœè¡¨æ ¼ä¸­æ²¡æœ‰å¯æå–çš„è´¢åŠ¡æŒ‡æ ‡ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
"""

# ================= å·¥å…·å‡½æ•° =================

llm_client = None

def get_llm_client():
    """è·å–æˆ–åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
    global llm_client
    if llm_client is None:
        if not LLM_API_KEY:
            raise ValueError("âŒ æœªè®¾ç½® LLM_API_KEY æˆ– DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        llm_client = OpenAI(
            api_key=LLM_API_KEY,
            base_url=LLM_API_BASE
        )
    return llm_client


def load_table_json(file_path: str) -> Tuple[Dict, List[Dict]]:
    """
    åŠ è½½æ ‡å‡†åŒ–è¡¨æ ¼JSONæ–‡ä»¶
    
    æ ¼å¼: {document: {...}, tables: [...]}
    
    Returns:
        (document_context, tables) å…ƒç»„
        - document_context: æ–‡æ¡£çº§å…¨å±€å…ƒæ•°æ®å­—å…¸
        - tables: è¡¨æ ¼åˆ—è¡¨
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if not isinstance(data, dict) or 'tables' not in data:
        raise ValueError(f"âŒ æ— æ•ˆçš„JSONæ ¼å¼: {file_path}\næœŸæœ›æ ¼å¼: {{document: {{...}}, tables: [...]}}")
    
    doc = data.get('document', {})
    tables = data.get('tables', [])
    
    # æ„é€ æ ‡å‡†åŒ–çš„ document_contextï¼ˆå…¨å±€æ–‡æ¡£ä¿¡æ¯ï¼‰
    stock_code = doc.get('stock_code', '')
    normalized_stock_code = stock_code.split('/')[0].strip() if '/' in stock_code else stock_code
    company_short = doc.get('company', '')
    
    # æ ‡å‡†åŒ–å…¬å¸åç§°
    company_name = _normalize_company_name(company_short, normalized_stock_code)
    
    doc_ctx = {
        'source': doc.get('source', ''),
        'company_name': company_name,  # ä½¿ç”¨æ ‡å‡†åŒ–åçš„ç®€ç§°
        'company_short': company_short,
        'stock_code': normalized_stock_code,  # ç›´æ¥ä½¿ç”¨å·²è§„èŒƒåŒ–çš„ä»£ç 
        'report_period': doc.get('report_period', ''),
        'report_type': doc.get('report_type', ''),
        'fiscal_year': doc.get('fiscal_year', ''),
        'data_scope': 'é›†å›¢'
    }
    
    return doc_ctx, tables


def extract_metrics_from_table(table_item: Dict, doc_ctx: Dict) -> List[Dict]:
    """
    ä½¿ç”¨ LLM ä» HTML è¡¨æ ¼ä¸­æå–ç»“æ„åŒ–è´¢åŠ¡æŒ‡æ ‡ï¼ˆæ”¯æŒåŸç”Ÿç»“æ„åŒ–è¾“å‡ºï¼‰
    
    Args:
        table_item: è¡¨æ ¼æ•°æ®é¡¹ï¼ŒåŒ…å« raw_html, summary, section ç­‰
        doc_ctx: å…¨å±€æ–‡æ¡£ä¸Šä¸‹æ–‡ä¿¡æ¯
        
    Returns:
        æå–çš„æŒ‡æ ‡åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« metric_name, metric_value, unit
    """
    try:
        raw_html = table_item.get('raw_html', '')
        if not raw_html or len(raw_html.strip()) < 20:
            return []
        
        # è·å–è¡¨æ ¼å…ƒä¿¡æ¯
        table_id = table_item.get('id', 'unknown')
        section = table_item.get('section', [])
        section_str = ' > '.join(section) if isinstance(section, list) else str(section)
        
        # æ’é™¤æ˜ç¡®ä¸åŒ…å«è´¢åŠ¡æŒ‡æ ‡çš„è¡¨æ ¼
        if any(kw in section_str for kw in EXCLUDE_TABLE_KEYWORDS):
            return []
        
        # è·å–å…¨å±€æ–‡æ¡£ä¿¡æ¯
        company_name = doc_ctx.get('company_short', doc_ctx.get('company_name', 'æœªçŸ¥'))
        stock_code = doc_ctx.get('stock_code', 'æœªçŸ¥')
        report_period = doc_ctx.get('report_period', 'æœªçŸ¥')
        
        # æ„å»º prompt
        prompt = METRIC_EXTRACTION_PROMPT.format(
            company_name=company_name,
            stock_code=stock_code,
            report_period=report_period,
            table_id=table_id,
            raw_html=raw_html
        )
        
        # è°ƒç”¨ LLMï¼ˆä½¿ç”¨åŸç”Ÿç»“æ„åŒ–è¾“å‡ºï¼‰
        client = get_llm_client()
        
        try:
            # å°è¯•ä½¿ç”¨åŸç”Ÿç»“æ„åŒ–è¾“å‡º
            completion = client.beta.chat.completions.parse(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                response_format=MetricsExtractionResult,
                temperature=0.1
            )
            
            result = completion.choices[0].message.parsed
            if result and result.metrics:
                metrics = [m.model_dump() for m in result.metrics]
            else:
                metrics = []
                
        except Exception as e:
            # é™çº§åˆ°ä¼ ç»Ÿæ–¹å¼
            print(f"   âš ï¸ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ–¹å¼: {e}")
            response = client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # è§£æ JSON
            json_match = re.search(r'\[[\s\S]*?\]', result_text)
            if json_match:
                metrics = json.loads(json_match.group())
            else:
                metrics = []
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        normalized_period = _normalize_report_period(report_period)
        # stock_code å’Œ company_name å·²åœ¨ doc_ctx ä¸­è§„èŒƒåŒ–ï¼Œç›´æ¥ä½¿ç”¨
        
        for m in metrics:
            m['stock_code'] = stock_code  # doc_ctx ä¸­å·²æ˜¯è§„èŒƒåŒ–åçš„å€¼
            m['company_name'] = company_name
            m['report_period'] = normalized_period
            m['source_table_id'] = table_id
        
        return metrics
        
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON è§£æå¤±è´¥ (è¡¨æ ¼ {table_item.get('id', 'unknown')}): {e}")
        return []
    except Exception as e:
        print(f"âŒ LLM æå–å¤±è´¥ (è¡¨æ ¼ {table_item.get('id', 'unknown')}): {e}")
        return []


def _normalize_company_name(entity: str, stock_code: str) -> str:
    """
    æ ‡å‡†åŒ–å…¬å¸åç§°ï¼Œä½¿ç”¨ç»Ÿä¸€çš„ç®€ç§°
    
    Args:
        entity: å®ä½“åç§°
        stock_code: è‚¡ç¥¨ä»£ç 
        
    Returns:
        æ ‡å‡†åŒ–åçš„å…¬å¸åç§°
    """
    # æ ¹æ®è‚¡ç¥¨ä»£ç æ˜ å°„
    code_to_name = {
        '600036.SH': 'æ‹›å•†é“¶è¡Œ',
        '601998.SH': 'ä¸­ä¿¡é“¶è¡Œ'
    }
    
    if stock_code in code_to_name:
        return code_to_name[stock_code]
    
    # æ ¹æ®å®ä½“åç§°æ˜ å°„
    name_map = {
        'CMB': 'æ‹›å•†é“¶è¡Œ',
        'æ‹›å•†é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸': 'æ‹›å•†é“¶è¡Œ',
        'æ‹›å•†é“¶è¡Œ': 'æ‹›å•†é“¶è¡Œ',
        'æ‹›è¡Œ': 'æ‹›å•†é“¶è¡Œ',
        'CITIC': 'ä¸­ä¿¡é“¶è¡Œ',
        'ä¸­ä¿¡é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸': 'ä¸­ä¿¡é“¶è¡Œ',
        'ä¸­ä¿¡é“¶è¡Œ': 'ä¸­ä¿¡é“¶è¡Œ',
        'ä¸­ä¿¡': 'ä¸­ä¿¡é“¶è¡Œ'
    }
    
    for key, name in name_map.items():
        if key in (entity or ''):
            return name
    
    return entity or 'æœªçŸ¥'


def _normalize_report_period(period: str) -> str:
    """
    æ ‡å‡†åŒ–æŠ¥å‘ŠæœŸæ ¼å¼
    ä¾‹å¦‚: "2025å¹´ç¬¬ä¸€å­£åº¦" -> "2025-Q1"
    """
    period = period.strip()
    
    # åŒ¹é… "2025å¹´ç¬¬ä¸€å­£åº¦" æˆ– "äºŒã€‡äºŒäº”å¹´ç¬¬ä¸€å­£åº¦"
    q_match = re.search(r'(äºŒã€‡\d{2}|20\d{2})å¹´ç¬¬([ä¸€äºŒä¸‰å››])å­£åº¦', period)
    if q_match:
        year_str = q_match.group(1)
        # è½¬æ¢ä¸­æ–‡å¹´ä»½
        if year_str.startswith('äºŒã€‡'):
            year = '20' + year_str[2:]
        else:
            year = year_str
        q_map = {'ä¸€': 'Q1', 'äºŒ': 'Q2', 'ä¸‰': 'Q3', 'å››': 'Q4'}
        quarter = q_map.get(q_match.group(2), 'Q1')
        return f"{year}-{quarter}"
    
    # åŒ¹é… "2025å¹´1-3æœˆ"
    month_match = re.search(r'(\d{4})å¹´(\d+)[-~](\d+)æœˆ', period)
    if month_match:
        year = month_match.group(1)
        end_month = int(month_match.group(3))
        if end_month == 3:
            return f"{year}-Q1"
        elif end_month == 6:
            return f"{year}-Q2"
        elif end_month == 9:
            return f"{year}-Q3"
        elif end_month == 12:
            return f"{year}-Q4"
    
    # åŒ¹é… "2025å¹´åŠå¹´åº¦"
    h_match = re.search(r'(\d{4})å¹´(åŠå¹´åº¦|ä¸ŠåŠå¹´)', period)
    if h_match:
        year = h_match.group(1)
        return f"{year}-H1"
    
    # åŒ¹é… "2025å¹´å¹´åº¦"
    y_match = re.search(r'(\d{4})å¹´(å¹´åº¦|åº¦)', period)
    if y_match:
        year = y_match.group(1)
        return f"{year}-FY"
    
    return period


def get_embedding(text: str) -> List[float]:
    """è°ƒç”¨ Ollama ç”Ÿæˆå‘é‡"""
    try:
        text = text.replace("\n", " ").strip()
        if not text:
            return np.zeros(EMBEDDING_DIM).tolist()
            
        response = ollama.embeddings(model=OLLAMA_MODEL, prompt=text)
        embedding = response.get('embedding')
        
        if not embedding or len(embedding) != EMBEDDING_DIM:
            print(f"âš ï¸ è­¦å‘Š: å‘é‡ç»´åº¦å¼‚å¸¸æˆ–ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
            return np.zeros(EMBEDDING_DIM).tolist()
            
        return embedding
    except Exception as e:
        print(f"âŒ Embedding è°ƒç”¨å¤±è´¥: {e}")
        return np.zeros(EMBEDDING_DIM).tolist()


def init_sqlite() -> sqlite3.Connection:
    """åˆå§‹åŒ– SQLite ç»“æ„åŒ–æŒ‡æ ‡åº“"""
    db_dir = os.path.dirname(SQL_DB_PATH)
    if db_dir and not os.path.exists(db_dir):
        os.makedirs(db_dir)
        print(f"   ğŸ“ åˆ›å»ºç›®å½•: {db_dir}")
    
    conn = sqlite3.connect(SQL_DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS financial_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            stock_code TEXT,
            company_name TEXT,
            report_period TEXT,
            metric_name TEXT,
            metric_value REAL,
            unit TEXT,
            source_table_id TEXT,
            UNIQUE(stock_code, report_period, metric_name)
        )
    ''')
    conn.commit()
    return conn


def init_milvus() -> Tuple[MilvusClient, str]:
    """åˆå§‹åŒ– Milvus å‘é‡åº“"""
    db_dir = os.path.dirname(MILVUS_DB_PATH)
    if db_dir and not os.path.exists(db_dir):
        os.makedirs(db_dir)
        print(f"   ğŸ“ åˆ›å»ºç›®å½•: {db_dir}")
    
    client = MilvusClient(uri=MILVUS_DB_PATH)
    collection_name = "financial_chunks"
    
    if not client.has_collection(collection_name):
        client.create_collection(
            collection_name=collection_name,
            dimension=EMBEDDING_DIM,
            metric_type="COSINE",
            auto_id=True
        )
    
    return client, collection_name


# ================= ä¸»æµç¨‹ =================

def main():
    parser = argparse.ArgumentParser(
        description="process_table_ingest.py: å¤„ç†æ ‡å‡†åŒ–è¡¨æ ¼æ•°æ®å¹¶å…¥åº“"
    )
    parser.add_argument("--input-file", type=str, required=True,
                        help="è¾“å…¥è¡¨æ ¼æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼ŒåŒ…å« document å’Œ tablesï¼‰")
    args = parser.parse_args()
    
    input_file = args.input_file
    
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {input_file}")
        return
    
    # 2. åˆå§‹åŒ–æ•°æ®åº“
    print("ğŸ› ï¸ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“...")
    sql_conn = init_sqlite()
    sql_cursor = sql_conn.cursor()
    milvus_client, collection_name = init_milvus()
    
    # 3. åŠ è½½è¡¨æ ¼æ•°æ®
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–è¡¨æ ¼æ–‡ä»¶: {input_file}")
    doc_ctx, tables = load_table_json(input_file)
    
    print(f"   âœ“ åŠ è½½äº† {len(tables)} ä¸ªè¡¨æ ¼")
    print(f"   âœ“ æ–‡æ¡£: {doc_ctx.get('company_short', 'Unknown')} - {doc_ctx.get('report_period', 'Unknown')}")
    
    # ---------------------------------------------------------
    # A. SQL Layer: ä½¿ç”¨ LLM ä»è¡¨æ ¼ä¸­æå–ç»“æ„åŒ–æŒ‡æ ‡
    # ---------------------------------------------------------
    print("ğŸ“Š æ­£åœ¨ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æŒ‡æ ‡...")
    
    all_metrics = []
    tables_processed = 0
    
    for table in tables:
        metrics = extract_metrics_from_table(table, doc_ctx)
        if metrics:
            all_metrics.extend(metrics)
            tables_processed += 1
            print(f"   âœ“ {table.get('id', 'unknown')}: æå–äº† {len(metrics)} ä¸ªæŒ‡æ ‡")
    
    # å»é‡ï¼šåŒä¸€å…¬å¸ã€åŒä¸€æŠ¥å‘ŠæœŸã€åŒä¸€æŒ‡æ ‡åç§°åªä¿ç•™ä¸€æ¡
    # å¦‚æœæœ‰å¤šä¸ªæ¥æºæå–äº†ç›¸åŒæŒ‡æ ‡ï¼Œä¼˜å…ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æ›´å‰é¢çš„è¡¨æ ¼ï¼‰
    seen = set()
    unique_metrics = []
    for m in all_metrics:
        key = (m['stock_code'], m['report_period'], m['metric_name'])
        if key not in seen:
            seen.add(key)
            unique_metrics.append(m)
        else:
            # è®°å½•è¢«å»é‡çš„æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            pass  # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ—¥å¿—
    
    # å†™å…¥ SQLite
    if unique_metrics:
        sql_records = [
            (
                m['stock_code'],
                m['company_name'],
                m['report_period'],
                m['metric_name'],
                m['metric_value'],
                m['unit'],
                m.get('source_table_id', 'unknown')
            )
            for m in unique_metrics
        ]
        
        sql_cursor.executemany('''
            INSERT OR REPLACE INTO financial_metrics 
            (stock_code, company_name, report_period, metric_name, metric_value, unit, source_table_id)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', sql_records)
        sql_conn.commit()
        print(f"   --> ä» {tables_processed} ä¸ªè¡¨æ ¼ä¸­æå–å¹¶å­˜å…¥ {len(unique_metrics)} æ¡æŒ‡æ ‡ï¼ˆå»é‡åï¼‰")
    else:
        print("   --> æœªæå–åˆ°ä»»ä½•æŒ‡æ ‡")
    
    # ---------------------------------------------------------
    # B. Vector + Keyword Layer: å¤„ç†å‘é‡å’Œ BM25
    # ---------------------------------------------------------
    print("ğŸ§  æ­£åœ¨å¤„ç†å‘é‡ Embedding å’Œ BM25 åˆ†è¯...")
    
    milvus_data = []
    bm25_corpus = []
    doc_map = []
    
    for table in tables:
        # ä½¿ç”¨ summary ä½œä¸ºæ£€ç´¢æ–‡æœ¬
        content = table.get('summary', '')
        if not content or len(content.strip()) < 10:
            continue
        
        # 1. ç”Ÿæˆå‘é‡
        vec = get_embedding(content)
        
        # 2. å‡†å¤‡ Milvus æ•°æ®ï¼ˆåŒ…å«å…¨å±€æ–‡æ¡£ä¿¡æ¯ï¼‰
        metadata_dict = {
            "source_id": str(table.get('id', 'unknown')),
            "type": "table",
            "page": str(table.get('page', 0)),
            # å…³é”®ï¼šè¡¨æ ¼å­˜ raw_htmlï¼Œè¿™æ˜¯ç»™ LLM çœ‹çš„åŸå§‹æ•°æ®
            "raw_data": table.get('raw_html', ''),
            "section": ' > '.join(table.get('section', [])) if isinstance(table.get('section'), list) else str(table.get('section', '')),
            # å…¨å±€æ–‡æ¡£ä¿¡æ¯ï¼ˆè´¯ç©¿æ‰€æœ‰æ•°æ®å±‚ï¼‰
            "company_name": doc_ctx.get('company_short', doc_ctx.get('company_name', '')),
            "stock_code": doc_ctx.get('stock_code', ''),
            "report_period": doc_ctx.get('report_period', ''),
            "report_type": doc_ctx.get('report_type', ''),
            "fiscal_year": doc_ctx.get('fiscal_year', ''),
            "source": doc_ctx.get('source', '')
        }
        
        entry = {
            "vector": vec,
            "text": content,  # å­˜æ‘˜è¦ç”¨äºè¯­ä¹‰æœç´¢
            "subject": "table_summary",
            "metadata": json.dumps(metadata_dict, ensure_ascii=False)
        }
        milvus_data.append(entry)
        
        # 3. å‡†å¤‡ BM25 æ•°æ®
        tokens = list(jieba.cut_for_search(content))
        bm25_corpus.append(tokens)
        doc_map.append(entry)
    
    # ---------------------------------------------------------
    # C. å†™å…¥å­˜å‚¨
    # ---------------------------------------------------------
    
    # 1. å†™å…¥ Milvusï¼ˆå»é‡ï¼‰
    if milvus_data:
        print(f"ğŸ’¾ æ­£åœ¨å†™å…¥ Milvus ({len(milvus_data)} æ¡æ•°æ®)...")
        
        # æ£€æŸ¥å·²å­˜åœ¨çš„æ•°æ® (ä½¿ç”¨ company_name + source_id ç»„åˆå»é‡)
        existing_keys = set()
        try:
            batch_size = 16384
            offset = 0
            while True:
                batch_data = milvus_client.query(
                    collection_name=collection_name,
                    filter="",
                    output_fields=["metadata"],
                    limit=batch_size,
                    offset=offset
                )
                if not batch_data:
                    break
                    
                for item in batch_data:
                    try:
                        metadata = json.loads(item.get('metadata', '{}'))
                        company = metadata.get('company_name', '')
                        source_id = metadata.get('source_id', '')
                        # ä½¿ç”¨å…¬å¸å+source_idç»„åˆä½œä¸ºå”¯ä¸€é”®
                        existing_keys.add(f"{company}::{source_id}")
                    except:
                        pass
                
                if len(batch_data) < batch_size:
                    break
                offset += batch_size
            
            if existing_keys:
                print(f"   â„¹ï¸ å‘ç° {len(existing_keys)} æ¡å·²å­˜åœ¨çš„æ•°æ®")
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•æ£€æŸ¥å·²å­˜åœ¨æ•°æ®: {e}")
        
        # è¿‡æ»¤é‡å¤æ•°æ® (åŸºäº company_name + source_id ç»„åˆ)
        new_data = []
        skipped = 0
        for entry in milvus_data:
            metadata = json.loads(entry['metadata'])
            company = metadata.get('company_name', '')
            source_id = metadata.get('source_id', '')
            key = f"{company}::{source_id}"
            if key not in existing_keys:
                new_data.append(entry)
            else:
                skipped += 1
        
        if new_data:
            res = milvus_client.insert(collection_name=collection_name, data=new_data)
            print(f"   --> æˆåŠŸå†™å…¥ {res['insert_count']} æ¡æ–°å‘é‡ï¼Œè·³è¿‡ {skipped} æ¡é‡å¤æ•°æ®")
        else:
            print(f"   --> æ‰€æœ‰æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ {skipped} æ¡é‡å¤æ•°æ®")
    
    # 2. æ„å»ºå¹¶ä¿å­˜ BM25ï¼ˆæ”¯æŒè¿½åŠ ï¼‰
    existing_doc_map = []
    if os.path.exists(BM25_INDEX_PATH):
        try:
            with open(BM25_INDEX_PATH, 'rb') as f:
                _, existing_doc_map = pickle.load(f)
            print(f"   â„¹ï¸ å‘ç°å·²æœ‰ BM25 ç´¢å¼•ï¼ŒåŒ…å« {len(existing_doc_map)} æ¡æ–‡æ¡£")
        except Exception as e:
            print(f"   âš ï¸ è¯»å–å·²æœ‰ BM25 ç´¢å¼•å¤±è´¥: {e}")
    
    if bm25_corpus or existing_doc_map:
        print("ğŸ“‘ æ­£åœ¨æ„å»º/æ›´æ–° BM25 ç´¢å¼•...")
        from rank_bm25 import BM25Okapi
        
        # æ£€æŸ¥é‡å¤ (ä½¿ç”¨ company_name + source_id ç»„åˆ)
        existing_keys_bm25 = set()
        for entry in existing_doc_map:
            try:
                metadata = json.loads(entry.get('metadata', '{}'))
                company = metadata.get('company_name', '')
                source_id = metadata.get('source_id', '')
                existing_keys_bm25.add(f"{company}::{source_id}")
            except:
                pass
        
        # è¿‡æ»¤é‡å¤æ•°æ® (åŸºäº company_name + source_id ç»„åˆ)
        filtered_corpus = []
        filtered_doc_map = []
        skipped_bm25 = 0
        
        for i, entry in enumerate(doc_map):
            try:
                metadata = json.loads(entry.get('metadata', '{}'))
                company = metadata.get('company_name', '')
                source_id = metadata.get('source_id', '')
                key = f"{company}::{source_id}"
                if key not in existing_keys_bm25:
                    filtered_corpus.append(bm25_corpus[i])
                    filtered_doc_map.append(entry)
                else:
                    skipped_bm25 += 1
            except:
                filtered_corpus.append(bm25_corpus[i])
                filtered_doc_map.append(entry)
        
        if skipped_bm25 > 0:
            print(f"   â„¹ï¸ BM25 è·³è¿‡ {skipped_bm25} æ¡é‡å¤æ•°æ®")
        
        # å‡†å¤‡å…¨é‡è¯­æ–™
        full_corpus = []
        
        # é‡æ–°åˆ†è¯æ—§æ•°æ®
        if existing_doc_map:
            print(f"   ...æ­£åœ¨é‡æ–°åˆ†è¯æ—§æ•°æ® ({len(existing_doc_map)} æ¡)...")
            for entry in existing_doc_map:
                full_corpus.append(list(jieba.cut_for_search(entry['text'])))
        
        full_corpus.extend(filtered_corpus)
        full_doc_map = existing_doc_map + filtered_doc_map
        
        bm25 = BM25Okapi(full_corpus)
        
        # ä¿å­˜ç´¢å¼•
        bm25_dir = os.path.dirname(BM25_INDEX_PATH)
        if bm25_dir and not os.path.exists(bm25_dir):
            os.makedirs(bm25_dir)
            print(f"   ğŸ“ åˆ›å»ºç›®å½•: {bm25_dir}")
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ BM25 ç´¢å¼•åˆ° {BM25_INDEX_PATH}...")
        with open(BM25_INDEX_PATH, 'wb') as f:
            pickle.dump((bm25, full_doc_map), f)
        print(f"   --> BM25 ç´¢å¼•ä¿å­˜å®Œæˆï¼ˆå…± {len(full_doc_map)} æ¡æ–‡æ¡£ï¼‰")
    
    # æ”¶å°¾
    sql_conn.close()
    print("\nğŸ‰ è¡¨æ ¼æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"   ğŸ“Š æå–æŒ‡æ ‡: {len(unique_metrics)} æ¡")
    print(f"   ğŸ§  å‘é‡æ•°æ®: {len(new_data) if milvus_data else 0} æ¡")
    print(f"   ğŸ“‘ BM25 ç´¢å¼•: {len(full_doc_map) if bm25_corpus or existing_doc_map else 0} æ¡")


if __name__ == "__main__":
    main()
